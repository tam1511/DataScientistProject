---
title: "Data Scientist Salary Prediction"
author: "Le Thi Thanh Tam"
date: "9/5/2022"
output: pdf_document
---

# Data Scienctist Salary Prediction


```{r}
rm(list=ls())
library(ggplot2)
# Load data
data <- read.csv('E:/ThanhTam_DA/Project/Prediction/Data Scientist Salary/data_model_2.csv')
```


```{r}
# Tranform columns
names(data)[names(data) == 'avg.salary.k.'] <- 'salary'
data$type.of.ownership = factor(data$type.of.ownership)
data$sector = factor(data$sector)
data$job.location = factor(data$job.location)
data$job_title_sim = factor(data$job_title_sim)
data$seniority_by_title = factor(data$seniority_by_title)
data$degree = factor(data$degree)

```

```{r}
# Best variable selection
library(leaps)
regfit.fwd = regsubsets(salary~., data, nvmax = 30, method = "forward")
fwd.summary = summary(regfit.fwd)
```


```{r}
fwd.summary$rsq
```

* we see that the R2 statistic increases from 14 %, when only one variable is included in the model, to almost 62 %, when all variables are included. As expected, the R2 statistic increases monotonically as more variables are included. 

```{r}
# Plot RSS, adjusted R squared, Cp and BIC for all of the models
par(mfrow = c(2,2))
plot(fwd.summary$rss, xlab = "Number of Variales", ylab = "RSS", type = "l")
plot(fwd.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted R sqd", type = "l")
# Identify the location of the maximum point of Adjusted R squared
which.max(fwd.summary$adjr2) # 30
# Plot a red dot to indicate the model with the largest adjusted R squared statistic
points(30, fwd.summary$adjr2[30], col = "red", cex = 2, pch = 20)

plot(fwd.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(fwd.summary$cp) #30
points(30, fwd.summary$cp[30], col = "red", cex = 2, pch = 20)

plot(fwd.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
which.min(fwd.summary$bic) # 18
points(18, fwd.summary$bic[18],col = "red", cex = 2, pch = 20 )
```

* We see the best model with the highest Adjusted R squared and lowest Cp is 30-variable model. 
```{r}
# Check the coefficient estimates associated with the model
coef(regfit.fwd, 2)
```

* For this data, the best one-variable model contains only **job_title_sim** for Data Analyst position, the best two-variable model additionally includes **seniority_by_title** with Senior level.

* However, to obtain the accuracy of that, we need to perform on the test set

* Next step I will use the the validation set approach to test and select the best model for this data and run the multiple linear regression 

```{r}
# write function for predict for regsubsets
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

set.seed(1)
train=sample(c(TRUE,FALSE), nrow(data), rep=TRUE)
test=(!train)

regfit.best=regsubsets(salary~., data=data[train,], nvmax=30, method = "forward")
test.mat=model.matrix(salary~., data=data[test,])
val.errors=rep(NA, 30)
for(i in 1:30){
  coefi=coef(regfit.best, id=i)
  pred=test.mat[,names(coefi)]%*%coefi
  val.errors[i]=mean((data$salary[test]-pred)^2)
}
which.min(val.errors) #29
val.errors # 995.8831 
# perform on the full data
regfit.best=regsubsets(salary~., data=data, nvmax=30,method="forward")
coef(regfit.best, 29) # select the best  29-variables model
# note we already perform the best subsets selection on the full data and select the best 29-variables model rather than simply using the variables that were obtained from the training set
###-___________\
#
```

* Our final best model after using validation set approach including 29 variables and the MSE is 995.8831

### Consider the result from multiple linear regression

```{r}
lm.fit=lm(salary~., data)
summary(lm.fit)

## Compute variance inflation factors
library(car)
vif(lm.fit)

# get list residual
res <- resid(lm.fit)

## normal distribution
### create a q-q plot
qqnorm(res)
qqline(res) # add a straight diagonal line to the plot
plot(density(res))
####-> we can see that the density plot roughly follows the bell shape (normal distribution)

## produce residual vs.fitted plot to visualizing heteroskedasticity
plot(fitted(lm.fit), res)
abline(0,0)
#--> we can see the data showing heteroscedasticity. the residuals are observed to have unequal variance

```

* we can see the data showing heteroscedasticity. the residuals are observed to have unequal variance

## Lasso

* Now I will perform the lasso _ the techniques for shirinking the regression coefficients towards zero. By using this technique, hope the coefficiet estimates can significantly reduce their variance for a more accurate prediction.

* Now I will perform the lasso in order to predict salary on this data

```{r}
# install.packages("glmnet")
library(glmnet)

x=model.matrix(salary~., data)[,-1] # automatically transforms any qualitative variables into dummy variables
y=data$salary

# split the sample data into a training set and a test set in order to estimate the test error of the lasso
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]

# now we fit a lasso model
grid=10^seq(10,-2,length=100)
lasso.mod=glmnet(x[train,], y[train], alpha=1, lambda = grid)
plot(lasso.mod) 
##-> we now see some of the coefficients will be exactly equal to zero

# perform cross-validation and compute the associated test error
set.seed(1)
cv.out=cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod, s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) # MSE = 762.544 
##-> This is substantially lower than the test set MSE of least squares 

```

* As expected, the lasso regression perform better than multiple linear regression compared MSE = 762.544 (mean square errors) in predicting the salary.

## RandomForest

```{r}
# install.packages("randomForest")
library(randomForest)
set.seed(1)
train=sample(1:nrow(data), nrow(data)/2)
data.test=data[-train,"salary"]
rf.data=randomForest(salary~., data=data, subset=train, mtry=6, importance=TRUE)
yhat.rf= predict(rf.data, newdata=data[-train,])
mean((yhat.rf-data.test)^2) # 575.7547
#plot of the importance measures
varImpPlot(rf.data)
```

* The results indicate that across all of the trees considerd in the random forest, the wealth level of the job title (job_title_sim) and job_location are by far the most important variables.

* The Random Forest model far outperformed the other approaches on the test and validation sets.



